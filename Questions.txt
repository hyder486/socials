**Machine Learning Interview Questions**


1. What is Machine learning?

Sol. A subset of Artificial Intelligence concerned with the development of statistical algorithms
     that can learn from data and perform tasks without explicit instructions.


2. Mention the difference between Data Mining and Machine learning?

Sol. Data Mining is concerned with the process of extracting information from huge amounts of data.
     It's about spotting useful patterns and trends, finding relatiosnhips between variables etc all 
     of which helps in gaining better insights for stakeholders. Machine Learning, on the other hand, 
     is concerned with the development of algorithms that can learn from data and perform tasks without
     explicit instructions.

     
3. What is ‘Overfitting’ in Machine learning?

Sol. Overfitting, in simple terms, is the inaccurate prediction of a model on the testing data.


4. Why overfitting happens?

Sol. Overfitting occurs when the machine learning model tries to cover all the data points
     or more than the required data points present in the given dataset. Because of this, 
     the model starts to learn from noise and inaccurate values present in the dataset, 
     and all these factors reduce the efficiency and accuracy of the model.


5. How can you avoid overfitting ?

Sol. Improving the quality of the training data reduces overfitting by focusing on 
     meaningful patterns. Increase the training data can improve the model’s ability
     to generalize to unseen data and reduce the likelihood of overfitting. Using 
     regularisation techniques such as Ridge and Lasso may help. Reduce model complexity.


6. What is inductive machine learning?

Sol. Inductive machine learning trains a model to generate predictions based on examples or observations. 
     During inductive learning, the model picks up knowledge from particular examples or instances and 
     generalizes it such that it can predict outcomes for brand-new data. When using inductive learning, 
     a rule or method is not explicitly programmed into the model. Instead, the model is trained to spot trends
     and connections in the input data and then utilize this knowledge to predict outcomes from fresh data. 
     Making a model that can precisely anticipate the result of subsequent instances is the aim of inductive learning. 


7. What are the five popular algorithms of Machine Learning?

Sol. Linear Regression, Logistic Regression, Decision Tree, Random Forest, SVM, KNN, K-means clustering.


8. What are the different Algorithm techniques in Machine Learning?

Sol. Supervised Learning, Unsupervised Learning, Semi-supervised Learning, Reinforcement Learning, Ensemble Learning,
     Neural Networks and Deep Learning, Transfer Learning.


9. What are the three stages to build the hypotheses or model in machine learning?

Sol. Understand the problem at hand, data needs and define success criteria.
     Collect, clean and pre-process the data for training.
     Model training and performance evaluation.
     Deployment and model monitoring.
     If required, iterate and adjust the model in production.
     
      
10. What is the standard approach to supervised learning?

Sol. Supervised learning uses labeled training dataset to train the model. This training dataset
     includes independent and correct dependent variables, which allow the model to learn over time.
     The algorithm measures its accuracy through the loss function, adjusting until the error has been
     sufficiently minimized.

 
11. What is ‘Training set’ and ‘Test set’?

Sol. train and test datasets are the two key concepts of machine learning, where the training dataset is used
     to fit the model, and the testing dataset is used to evaluate the model. Both are subsets of the original
     dataset.


12. List down various approaches for machine learning?

Sol. There are different ways for a machine to learn. Different approaches to ML are commonly distinguished by 
     the kinds of problems they try to solve. Broadly, we can divide machine learning into three subareas:  

     Supervised learning
     Unsupervised learning
     Reinforcement learning
     Although this might look like a neat categorization, it's not always easy to place a particular method.

 
13. What is not Machine Learning?

Sol. Anything that follows predefined rules or does not learn from data is not machine learning. Machine Learning 
     is different because it has the ability to learn from data, generalise patterns, and improve over time.
     Traditional Rule-Based systems, Hardcoded Algorithms, Simple statistical methods, Automation without learning are
     some examples for things which are not machine learning.

  
14. Explain what is the function of ‘Unsupervised Learning’?

Sol. The goal of unsupervised learning is to find the underlying structure of dataset, group that data according to similarities,
     and represent that dataset in a compressed format. Models are not supervised using training dataset. Instead, models themselves
     find the hidden patterns and insights from the given data. It can be compared to learning which takes place in the human brain
     while learning new things. 


15. Explain what is the function of ‘Supervised Learning’?

Sol. Supervised machine learning attempts to explain the behavior of the target as a function of a set of independent attributes
     or predictors. Supervised learning generally results in predictive models.

 
16. What is algorithm independent machine learning?

Sol. principles and methods in machine learning that are not tied to any specific algorithm or model. They can be broadly applied on 
     a wide variety of algorithms. These are general techinques that improve the performance of a model. Eg: feature engineering,
     cross validation, hyperparameter tuning, data preprocessing.

 
17. What is the difference between artificial learning and machine learning?

Sol. Artificial intelligence (AI) is an umbrella term for different strategies and techniques you can use to make machines
     mimic human intelligence. AI includes everything from smart assistants like Alexa to robotic vacuum cleaners and self-driving cars.
     Machine learning (ML) is one among many other branches of AI. ML is the science of developing algorithms and statistical models
     that computer systems use to perform complex tasks without explicit instructions. The systems rely on patterns and inference instead.
     Computer systems use ML algorithms to process large quantities of historical data and identify data patterns. While machine learning
     is AI, not all AI activities are machine learning.

 
18. What is classifier in machine learning?

Sol. In machine learning, a classifier is an algorithm that automatically sorts or categorizes data into one or more "classes."
     Targets, labels, and categories are all terms used to describe classes.

 
19. What are the advantages of Naive Bayes?

Sol. The following are some of the benefits of the Naive Bayes classifier:

     It is simple and easy to implement
     It doesn’t require as much training data
     It handles both continuous and discrete data
     It is highly scalable with the number of predictors and data points
     It is fast and can be used to make real-time predictions
     It is not sensitive to irrelevant features

     Disadvantages of Naive Bayes:

     Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life.
     This limits the applicability of this algorithm in real-world use cases.
     This algorithm faces the ‘zero-frequency problem’ where it assigns zero probability to a categorical variable
     whose category in the test data set wasn’t available in the training dataset. It would be best if you used
     a smoothing technique to overcome this issue.
     Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously.


20. What is Inductive Logic Programming in Machine Learning?

Sol. Inductive Logic Programming (ILP), is a subfield of machine learning that learns computer programs from data,
     where the programs and data are logic programs. It may also be explained as a form of
     supervised machine learning which uses logic programming as a uniform representation for background knowledge,
     examples, and induced theories. ILP is preferred over other machine learning approaches
     because of its easy comprehensibility, intelligibility, and ability to include additional information in the learning problem.

 
21. What is Model Selection in Machine Learning?

Sol. Model selection is the process of deciding which algorithm and model architecture is best suited
     for a particular dataset. It entails contrasting various models, assessing their efficacy,
     and choosing the one that most effectively addresses the problem.

 
22. What are the two methods used for the calibration in Supervised Learning?

Sol. Platt scaling and isotonic regression, which adjust the model’s probability outputs to align better with true outcome frequencies.


23. Which method is frequently used to prevent overfitting?

Sol. One of the most frequently used methods to prevent overfitting is regularization.

 
24. Why instance based learning algorithm sometimes referred as Lazy learning
    algorithm?

Sol. The Machine Learning systems which are categorized as instance-based learning are the systems
     that learn the training examples by heart and then generalizes to new instances based on some similarity measure.
     It is called instance-based because it builds the hypotheses from the training instances.
     It is also known as memory-based learning or lazy-learning (because they delay processing until a new instance must be classified). 


25. What are the two classification methods that SVM ( Support Vector Machine) can
    handle?

Sol. Binary and multi-class


26. What is ensemble learning?

Sol. Ensemble learning is a machine learning technique that aggregates two or more learners (e.g. regression models, neural networks)
     in order to produce better predictions. In other words, an ensemble model combines several individual models
     to produce more accurate predictions than a single model alone. At times, sources may refer to this technique as
     committee-based learning. Ensemble learning rests on the principle that a collective of learners yield
     greater overall accuracy than an individual learner.


27. When to use ensemble learning?

Sol. when you want to enhance performance, stability, and robustness of your predictions, especially in complex or challenging datasets. 


28. What are the two paradigms of ensemble methods?

Sol. Bagging and Boosting

 
29. What is the general principle of an ensemble method and what is bagging and
    boosting in ensemble method?

Sol. Ensemble learning rests on the principle that a collective of learners yield
     greater overall accuracy than an individual learner. Bagging trains each base model
     independently and in parallel, using bootstrap sampling to create multiple subsets
     of the training data. The final prediction is then made by averaging the predictions of all base models.
     Bagging focuses on reducing variance and overfitting by creating diverse models. In contrast, 
     boosting trains models sequentially, with each subsequent model focusing on correcting the errors
     made by the previous ones. Boosting adjusts the weights of training instances to prioritize
     difficult-to-classify instances, thus reducing bias and improving predictive accuracy. 
     The final prediction combines the outputs of all models, usually through weighted voting or averaging.
     Additionally, while bagging is relatively simple and easy to parallelize, boosting is more complex
     due to its sequential nature and may be more prone to overfitting if not properly controlled.

 
30. What is bias-variance decomposition of classification error in ensemble method?

Sol. Bias-Variance decomposition refers to the process of breaking down the total expected error of
     a classifier into two components: bias and variance. The bias measures how well the
     learning method matches the problem, while the variance accounts for errors due to
     the particular training set used. 


31. What is an Incremental Learning algorithm in ensemble?

Sol. Incremental learning is a methodology of machine learning where an AI model learns and
     enhances its knowledge progressively, without forgetting previously acquired information. 
     In essence, it imitates human learning patterns by acquiring new information over time,
     while maintaining and building upon previous knowledge. 
     Incremental learning is crucial in scenarios where data arrives in sequential order or
     where the storage of all data for processing is not feasible.

 
32. What is PCA, KPCA and ICA used for?

Sol.  PCA linearly transforms the original inputs into new uncorrelated features.
      KPCA is a nonlinear PCA developed by using the kernel method. In ICA,
     the original inputs are linearly transformed into features which are mutually statistically independent.

 
33. What is dimension reduction in Machine Learning?

Sol. Dimensionality reduction is the process of reducing the number of features
    (or dimensions) in a dataset while retaining as much information as possible. 
    This can be done for a variety of reasons, such as to reduce the complexity of a model,
    to improve the performance of a learning algorithm, or to make it easier to visualize the data.
    There are several techniques for dimensionality reduction, including principal component analysis (PCA),
    singular value decomposition (SVD), and linear discriminant analysis (LDA). Each technique uses
    a different method to project the data onto a lower-dimensional space while preserving important information. 


34. What are support vector machines?

Sol. Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, 
     which is used for Classification as well as Regression problems. However, primarily,
     it is used for Classification problems in Machine Learning. The goal of the SVM algorithm
     is to create the best line or decision boundary that can segregate n-dimensional space
     into classes so that we can easily put the new data point in the correct category in the future.
     This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors
     that help in creating the hyperplane. These extreme cases are called as support vectors,
     and hence algorithm is termed as Support Vector Machine.

 
35. Differentiate between inductive learning and deductive learning?

Sol. Inductive machine learning starts with specific examples and then generalizes to broader principles.
     It's like learning from specific cases and then applying that knowledge to similar situations. For example,
     if you show a computer many pictures of cats and label them as "cats," it will learn to recognize
     common features shared by all cats, such as pointy ears and whiskers. Then, when it encounters a new picture
     of a cat it hasn't seen before, it can use its learned knowledge to identify it as a cat. On the other hand,
     deductive machine learning begins with general principles or rules and then applies them to specific cases
     to make predictions or decisions. It's like starting with a set of rules and then using them to classify or
     make decisions about specific instances. For instance, if you provide a computer with rules for identifying
     animals based on characteristics like having four legs and fur, it can use these rules to classify new animals
     it encounters as either cats or dogs. In summary, inductive machine learning learns from specific examples
     to derive general principles, while deductive machine learning starts with general principles to make decisions
     about specific cases.

  
36. What is the difference between Data Mining and Machine Learning?
37. Differentiate supervised and unsupervised machine learning.

Sol. The main distinction between the two approaches is the use of labeled data sets. To put it simply,
     supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not.
     In supervised learning, the algorithm “learns” from the training data set by iteratively making predictions
     on the data and adjusting for the correct answer. While supervised learning models tend to be more accurate
     than unsupervised learning models, they require upfront human intervention to label the data appropriately.
     For example, a supervised learning model can predict how long your commute will be based on the time of day,
     weather conditions and so on. But first, you must train it to know that rainy weather extends the driving time.
     Unsupervised learning models, in contrast, work on their own to discover the inherent structure of unlabeled data.
     Note that they still require some human intervention for validating output variables. For example,
     an unsupervised learning model can identify that online shoppers often purchase groups of products at the same time.
     However, a data analyst would need to validate that it makes sense for a recommendation engine to
     group baby clothes with an order of diapers, applesauce, and sippy cups.Goals: In supervised learning, the goal is to
     predict outcomes for new data. You know up front the type of results to expect. With an unsupervised learning algorithm,
     the goal is to get insights from large volumes of new data. The machine learning itself determines what is different
     or interesting from the data set.
     Applications: Supervised learning models are ideal for spam detection, sentiment analysis, weather forecasting
     and pricing predictions, among other things. In contrast, unsupervised learning is a great fit for anomaly detection,
     recommendation engines, customer personas and medical imaging.
     Complexity: Supervised learning is a simple method for machine learning, typically calculated by using programs
     like R or Python. In unsupervised learning, you need powerful tools for working with large amounts of
     unclassified data. Unsupervised learning models are computationally complex because they need a large training set
     to produce intended outcomes.
     Drawbacks: Supervised learning models can be time-consuming to train, and the labels for input and output variables
     require expertise. Meanwhile, unsupervised learning methods can have wildly inaccurate results unless you have
     human intervention to validate the output variables. 


38. How does Machine Learning differ from Deep Learning?

Sol. Machine learning is a subset of artificial intelligence. In turn, deep learning is a subset of machine learning.
     Essentially, all deep learning is machine learning, and all machine learning is artificial intelligence,
     but not all artificial intelligence is machine learning.

 
39. How is KNN different from k-means?

Sol. KNN is a supervised learning algorithm used for classification and regression. On the contrary,
     K-means is an unsupervised learning algorithm used for clustering. Let us discuss some of the
     differences between the KNN and K-means clustering algorithms.
     Objective: We use the KNN algorithm for classification and regression tasks. The K-Means algorithm
     is used for clustering.
     Supervision: KNN is a supervised machine learning algorithm. KMeans is an unsupervised machine learning algorithm.
     Input:  To train a KNN model, we need a dataset with all the data points having class labels. 
     For training a K-means clustering model, we don’t need any such information.
     Output: We use the KNN algorithm to predict the class label of a new data point. On the other hand,
     we use the KMeans algorithm to find patterns in a given dataset by grouping data points into clusters.
     Parameter: The KNN algorithm requires the choice of the number of nearest neighbors as its input parameter.
     The KMeans clustering algorithm requires the number of clusters as an input parameter.

 
40. What are the different types of Algorithm methods in Machine Learning?
41. What do you understand by Reinforcement Learning technique?

Sol. Through a series of Trial and Error methods, an agent keeps learning continuously in an interactive environment
     from its own actions and experiences. The only goal of it is to find a suitable action model which would increase
     the total cumulative reward of the agent. It learns via interaction and feedback.


42. What is the trade-off between bias and variance?

Sol. The bias-variance tradeoff implies that as we increase the complexity of a model, its variance decreases,
     and its bias increases. Conversely, as we decrease the model's complexity, its variance increases,
     but its bias decreases. The goal is to find the right balance between these two aspects to create a model
     that performs well on new, unseen data.

 
43. How do classification and regression differ?

Sol. Regression and Classification algorithms are Supervised Learning algorithms. Both the algorithms are used
     for prediction in Machine learning and work with the labeled datasets. But the difference between both
     is how they are used for different machine learning problems. The main difference between Regression
     and Classification algorithms that Regression algorithms are used to predict the continuous values
     such as price, salary, age, etc. and Classification algorithms are used
     to predict/Classify the discrete values such as Male or Female, True or False, Spam or Not Spam, etc. 


44. What are the three stages of building the hypotheses or model in machine
    learning?
45. Describe 'Training set' and 'training Test'.
46. What are the common ways to handle missing data in a dataset?

Sol. Deleting the column with missing data, Deleting the row with missing data,
     Imputing missing values with mean/median, Forward Fill and Backward Fill

 
47. What are the necessary steps involved in Machine Learning Project?
48. Describe Precision and Recall?

Sol. Precision is defined as the ratio of correctly classified positive samples (True Positive) to
     a total number of classified positive samples (either correctly or incorrectly).
     Precision = True Positive/True Positive + False Positive
     
     The recall is calculated as the ratio between the numbers of Positive samples correctly classified
     as Positive to the total number of Positive samples. The recall measures the model's ability to
     detect positive samples. The higher the recall, the more positive samples detected.
     Recall = True Positive/True Positive + False Negative  


49. What do you understand by Decision Tree in Machine Learning?

Sol. Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems,
     but mostly it is preferred for solving Classification problems. It is a tree-structured classifier,
     where internal nodes represent the features of a dataset, branches represent the decision rules and
     each leaf node represents the outcome. In a Decision tree, there are two nodes, which are the Decision Node
     and Leaf Node. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes
     are the output of those decisions and do not contain any further branches. The decisions or the test are 
     performed on the basis of features of the given dataset.


50. What do you understand by algorithm independent machine learning?
51. Describe the classifier in machine learning.
52. What is SVM in machine learning? What are the classification methods that SVM
    can handle?
53. What do you understand by the Confusion Matrix?

Sol. A confusion matrix is a matrix that summarizes the performance of a machine learning model on a set of test data.
     It is a means of displaying the number of accurate and inaccurate instances based on the model’s predictions.
     It is often used to measure the performance of classification models, which aim to predict a categorical label
     for each input instance. The matrix displays the number of instances produced by the model on the test data.

     True Positive (TP): The model correctly predicted a positive outcome (the actual outcome was positive).
     True Negative (TN): The model correctly predicted a negative outcome (the actual outcome was negative).
     False Positive (FP): The model incorrectly predicted a positive outcome (the actual outcome was negative).
     Also known as a Type I error.
     False Negative (FN): The model incorrectly predicted a negative outcome (the actual outcome was positive).
     Also known as a Type II error.

 
54. Explain True Positive, True Negative, False Positive, and False Negative in
    Confusion Matrix with an example.
55. What according to you, is more important between model accuracy and model
    performance?

Sol. While accuracy is an important and commonly used metric, model performance 
     is generally more important because it provides a holistic view of how the model behaves
     under different conditions and across various metrics. Performance measures reflect how well
     the model aligns with the real-world goals and requirements of the task, ensuring that
     the model is not only correct but also useful in practice.


56. What is Bagging and Boosting?
57. What are the similarities and differences between bagging and boosting in
    Machine Learning?

Sol. 
58. What do you understand by Cluster Sampling?
59. What do you understand by the F1 score?
60. How is a decision tree pruned?
61. What are the Recommended Systems?
62. When does regularization become necessary in Machine Learning?
63. What is Regularization? What kind of problems does regularization solve?
64. Why do we need to convert categorical variables into factor? Which functions are
    used to perform the conversion?
65. Do you think that treating a categorical variable as a continuous variable would
    result in a better predictive model?
66. How is machine learning used in day-to-day life?
67. How Do You Handle Missing or Corrupted Data in a Dataset?
68. How Can You Choose a Classifier Based on a Training Set Data Size?
69. What Are the Applications of Supervised Machine Learning in Modern
    Businesses?
70. What is Semi-supervised Machine Learning?
71. Compare K-means and KNN Algorithms.
72. What Is ‘naive’ in the Naive Bayes Classifier?
73. How Will You Know Which Machine Learning Algorithm to Choose for Your
    Classification Problem?
74. How is Amazon Able to Recommend Other Things to Buy? How Does the
    Recommendation Engine Work?
75. When Will You Use Classification over Regression?
76. How Do You Design an Email Spam Filter?
77. What is a Random Forest?
78. What is Pruning in Decision Trees, and How Is It Done?
79. Briefly Explain Logistic Regression.
80. Explain the K Nearest Neighbor Algorithm.
81. What is Kernel SVM?
82. What Are Some Methods of Reducing Dimensionality?
83. What is Principal Component Analysis?
84. What do you understand by Type I vs Type II error?
85. Explain Correlation and Covariance?
86. What are Support Vectors in SVM?
87. What is Cross-Validation?
88. What are the different methods to split a tree in a decision tree algorithm?
89. How does the Support Vector Machine algorithm handle self-learning?90. What are the assumptions you need to take before starting with linear
    regression?
91. What is the difference between Lasso and Ridge regression?
92. What is Entropy in Machine Learning?
93. What is Epoch in Machine Learning?
94. Differentiate between Classification and Regression in Machine Learning
95. How is the suitability of a Machine Learning Algorithm determined for a
    particular problem?
96. What is ROC Curve and what does it represent?
97. Both being Tree-based Algorithms, how is Random Forest different from
    Gradient Boosting Machine (GBM)?
98. What do you understand about the P-value?
99. Suppose you found that your model is suffering from high variance. Which
    algorithm do you think could handle this situation and why?
100. What is Rescaling of Data and how is it done?
